{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from gensim.models import Word2Vec\n",
    "!pip install html2text\n",
    "import html2text as h2t\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# file path could be different, this is from our google colab\n",
    "labeled_train_data = pd.read_csv(\"/content/labeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\n",
    "unlabeled_train_data = pd.read_csv(\"/content/testData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\n",
    "test_data = pd.read_csv(\"/content/unlabeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def review_to_token(review_raw):\n",
    "  untagged_text = h2t.html2text(review_raw)\n",
    "  alltext = re.sub(\"[^a-zA-Z]\", \" \", untagged_text)\n",
    "  lowercase_text = alltext.lower()\n",
    "  tokenized = word_tokenize(lowercase_text)\n",
    "  return tokenized\n",
    "\n",
    "def remove_stop_words(tokenized_review):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  cleaned_review = []\n",
    "  for word in tokenized_review:\n",
    "    if word not in stop_words:\n",
    "      cleaned_review.append(word)\n",
    "  return(cleaned_review)\n",
    "\n",
    "labeled_reviews = labeled_train_data['review'].tolist()\n",
    "unlabeled_reviews = unlabeled_train_data['review'].tolist()\n",
    "cleaned_reviews = []\n",
    "for review in labeled_reviews:\n",
    "  tokenized_review = review_to_token(review)\n",
    "  stop_words_removed = remove_stop_words(tokenized_review)\n",
    "  cleaned_reviews.append(stop_words_removed)\n",
    "\n",
    "def word2vec_func(reviews, vector_size=100, window=10, min_count=1, sg=0):\n",
    "  w2v_model = Word2Vec(reviews, vector_size = vector_size, window = window, min_count = min_count, sg = sg)\n",
    "  return(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec_func(cleaned_reviews)\n",
    "\n",
    "def vector_average(tokenized_review, w2v_model):\n",
    "  raw_vectors = []\n",
    "  for word in tokenized_review:\n",
    "    if(word in w2v_model.wv):\n",
    "      raw_vectors.append(w2v_model.wv[word]) # w2v_model[word] -> w2v_model.wv[word]\n",
    "  return np.mean(raw_vectors, axis = 0)\n",
    "\n",
    "review_vectors = [vector_average(review, w2v_model) for review in cleaned_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array(review_vectors)\n",
    "y = labeled_train_data['sentiment']\n",
    "\n",
    "x_train, x_test, y_train, y_test = tts(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ran_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "ran_forest.fit(x_train, y_train)\n",
    "\n",
    "y_predict = ran_forest.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_reviews = test_data['review'].tolist()\n",
    "cleaned_test_reviews = []\n",
    "for i in test_reviews:\n",
    "  tokenized_review = review_to_token(i)\n",
    "  stop_words_removed = remove_stop_words(tokenized_review)\n",
    "  cleaned_test_reviews.append(stop_words_removed)\n",
    "\n",
    "test_vector = [vector_average(review, w2v_model) for review in cleaned_test_reviews]\n",
    "test_vector = np.array(test_vector)\n",
    "\n",
    "test_predictions = ran_forest.predict(test_vector)\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'sentiment': test_predictions\n",
    "})\n",
    "final_df.to_csv(\"submission478.csv\", index = False)\n",
    "\n",
    "final_df.head(5)\n",
    "\n",
    "# Examining the counts of the sentiment type in each dataframe \n",
    "print(final_df['sentiment'].value_counts())\n",
    "print(labeled_train_data['sentiment'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
